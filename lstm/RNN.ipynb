{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate, Reshape, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Yee Ann/NUS/DSA4266/TruthSeeker2023/Features_For_Traditional_ML_Techniques.csv\") as file:\n",
    "    df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with s single unique value\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df = df.loc[:, df.nunique() > 1]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Convert boolean columns to 0 and 1\n",
    "df['majority_target'] = df['majority_target'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "# Combine the text fields (statement, tweet) into one\n",
    "df['text_combined'] = df['statement'] + ' ' + df['tweet']\n",
    "\n",
    "# Tokenize the combined text field\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['text_combined'])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_text = tokenizer.texts_to_sequences(df['text_combined'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure they are the same length\n",
    "max_length = 100  # Set max length of sequences\n",
    "X_text = pad_sequences(X_text, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Separate the numerical features\n",
    "numerical_features = df.select_dtypes(['int64', 'int32', 'float64']).columns.tolist()\n",
    "numerical_features.remove('majority_target')\n",
    "X_num = df[numerical_features].values\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text sequences and numerical features into one dataset\n",
    "X_combined = np.hstack((X_text, X_num_scaled))\n",
    "\n",
    "# Define the target variable\n",
    "y = df['majority_target'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=128, input_length=max_length))  # Embedding layer for text\n",
    "model.add(LSTM(128, return_sequences=True))  # LSTM layer\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(LSTM(64, return_sequences=False))  # Another LSTM layer\n",
    "model.add(Dropout(0.5))  # More regularization\n",
    "model.add(Dense(32, activation='relu'))  # Dense layer for additional complexity\n",
    "model.add(Dropout(0.5))  # Additional Dropout\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_use = (y_pred > 0.5).astype(int)\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_use)\n",
    "print(f\"RNN Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_use, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix to understand predictions\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentence transformer model\n",
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate sentence embeddings for the 'statement' column\n",
    "X_statement_embeddings = model_sbert.encode(df['statement'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# Generate sentence embeddings for the 'tweet' column\n",
    "X_tweet_embeddings = model_sbert.encode(df['tweet'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "numerical_features = df.select_dtypes(['int64', 'int32', 'float64']).columns.tolist()\n",
    "numerical_features.remove('majority_target')\n",
    "X_num = df[numerical_features].values\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for statement, tweet, and numerical features\n",
    "input_statement = Input(shape=(X_statement_embeddings.shape[1],))\n",
    "input_tweet = Input(shape=(X_tweet_embeddings.shape[1],))\n",
    "input_numerical = Input(shape=(X_num_scaled.shape[1],))\n",
    "\n",
    "# Reshape to 3D for LSTM (adding a dimension for timesteps)\n",
    "reshaped_statement = Reshape((1, X_statement_embeddings.shape[1]))(input_statement)  # Shape: (batch_size, 1, 384)\n",
    "reshaped_tweet = Reshape((1, X_tweet_embeddings.shape[1]))(input_tweet)  # Shape: (batch_size, 1, 384)\n",
    "\n",
    "# LSTM for statement embeddings\n",
    "lstm_statement = LSTM(128, return_sequences=False)(reshaped_statement)\n",
    "\n",
    "# LSTM for tweet embeddings\n",
    "lstm_tweet = LSTM(128, return_sequences=False)(reshaped_tweet)\n",
    "\n",
    "# Combine LSTM outputs and numerical features\n",
    "combined = Concatenate()([lstm_statement, lstm_tweet, input_numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirection RNN (run this instead of the box on top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for statement, tweet, and numerical features\n",
    "input_statement = Input(shape=(X_statement_embeddings.shape[1],))\n",
    "input_tweet = Input(shape=(X_tweet_embeddings.shape[1],))\n",
    "input_numerical = Input(shape=(X_num_scaled.shape[1],))\n",
    "\n",
    "# Reshape to 3D for LSTM (adding a dimension for timesteps)\n",
    "reshaped_statement = Reshape((1, X_statement_embeddings.shape[1]))(input_statement)  # Shape: (batch_size, 1, 384)\n",
    "reshaped_tweet = Reshape((1, X_tweet_embeddings.shape[1]))(input_tweet)  # Shape: (batch_size, 1, 384)\n",
    "\n",
    "# Bidirectional LSTM for statement embeddings\n",
    "bidirectional_lstm_statement = Bidirectional(LSTM(128, return_sequences=False))(reshaped_statement)\n",
    "\n",
    "# Bidirectional LSTM for tweet embeddings\n",
    "bidirectional_lstm_tweet = Bidirectional(LSTM(128, return_sequences=False))(reshaped_tweet)\n",
    "\n",
    "# Combine LSTM outputs and numerical features\n",
    "combined = Concatenate()([bidirectional_lstm_statement, bidirectional_lstm_tweet, input_numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layers\n",
    "# x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "x = Dense(64, activation='relu')(combined)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_statement, input_tweet, input_numerical], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_statement, X_test_statement, X_train_tweet, X_test_tweet, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    X_statement_embeddings, X_tweet_embeddings, X_num_scaled, df['majority_target'], test_size=0.3, random_state=42)\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "history = model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_statement, X_test_tweet, X_test_num], y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([X_test_statement, X_test_tweet, X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_use = (y_pred > 0.5).astype(int)\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_use)\n",
    "print(f\"RNN Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_use, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix to understand predictions\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tokenizer gives 0.9560\n",
    "# the one with regularizer gives 0.9571 accuracy\n",
    "# with only dropout is 0.9576\n",
    "# with BatchNormalization is 0.9576\n",
    "# with reduce learning rate and early stopping is also 0.9576\n",
    "# Bi-directional RNN is 0.9576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1', 'keras_tensor_2']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_13', 'keras_tensor_14', 'keras_tensor_15']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_26', 'keras_tensor_27', 'keras_tensor_28']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_39', 'keras_tensor_40', 'keras_tensor_41']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_52', 'keras_tensor_53', 'keras_tensor_54']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "Cross-Validation Results (5 folds):\n",
      "Accuracy: 0.9573 ± 0.0006\n",
      "Precision: 0.9588 ± 0.0021\n",
      "Recall: 0.9580 ± 0.0014\n",
      "F1 Score: 0.9584 ± 0.0006\n",
      "AUC: 0.9588 ± 0.0002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list, precision_list, recall_list, f1_list, auc_list = [], [], [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(X_statement_embeddings):\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train_statement, X_val_statement = X_statement_embeddings[train_index], X_statement_embeddings[val_index]\n",
    "    X_train_tweet, X_val_tweet = X_tweet_embeddings[train_index], X_tweet_embeddings[val_index]\n",
    "    X_train_num, X_val_num = X_num_scaled[train_index], X_num_scaled[val_index]\n",
    "    y_train, y_val = df['majority_target'].iloc[train_index], df['majority_target'].iloc[val_index]\n",
    "    \n",
    "    # Define inputs and model\n",
    "    input_statement = Input(shape=(X_statement_embeddings.shape[1],))\n",
    "    input_tweet = Input(shape=(X_tweet_embeddings.shape[1],))\n",
    "    input_numerical = Input(shape=(X_num_scaled.shape[1],))\n",
    "    \n",
    "    # Reshape to 3D for LSTM (adding a dimension for timesteps)\n",
    "    reshaped_statement = Reshape((1, X_statement_embeddings.shape[1]))(input_statement)\n",
    "    reshaped_tweet = Reshape((1, X_tweet_embeddings.shape[1]))(input_tweet)\n",
    "    \n",
    "    # LSTM for statement and tweet embeddings\n",
    "    lstm_statement = LSTM(128, return_sequences=False)(reshaped_statement)\n",
    "    lstm_tweet = LSTM(128, return_sequences=False)(reshaped_tweet)\n",
    "    \n",
    "    # Combine LSTM outputs and numerical features\n",
    "    combined = Concatenate()([lstm_statement, lstm_tweet, input_numerical])\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Define and compile model\n",
    "    model = Model(inputs=[input_statement, input_tweet, input_numerical], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    y_pred_prob = model.predict([X_val_statement, X_val_tweet, X_val_num])\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred_prob)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "# Display the results for each metric\n",
    "print(f\"Cross-Validation Results ({n_folds} folds):\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_list):.4f} ± {np.std(accuracy_list):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_list):.4f} ± {np.std(precision_list):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_list):.4f} ± {np.std(recall_list):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation Results (5 folds):\n",
    "Accuracy: 0.9573 ± 0.0006\n",
    "Precision: 0.9588 ± 0.0021\n",
    "Recall: 0.9580 ± 0.0014\n",
    "F1 Score: 0.9584 ± 0.0006\n",
    "AUC: 0.9588 ± 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Robustness: The model is robust and performs consistently well across different data splits, showing it has generalized well to the dataset.\n",
    "\n",
    "Performance Ceiling: Since the accuracy, F1 score, and AUC are all very close to 0.96, it’s likely you’ve reached a performance ceiling with this dataset and feature set. Additional tuning may not yield significant improvements because the model seems to have effectively captured the key patterns for distinguishing fake from real news.\n",
    "\n",
    "Balanced Metrics: The high scores in precision and recall, along with a closely matching F1 score, indicate that the model maintains a good balance between identifying both real and fake news accurately, with a strong AUC showing good separation between classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
