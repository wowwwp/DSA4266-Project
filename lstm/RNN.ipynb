{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate, Reshape, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Yee Ann/NUS/DSA4266/TruthSeeker2023/Features_For_Traditional_ML_Techniques.csv\") as file:\n",
    "    df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with s single unique value\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df = df.loc[:, df.nunique() > 1]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Convert boolean columns to 0 and 1\n",
    "df['majority_target'] = df['majority_target'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "# Combine the text fields (statement, tweet) into one\n",
    "df['text_combined'] = df['statement'] + ' ' + df['tweet']\n",
    "\n",
    "# Tokenize the combined text field\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['text_combined'])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_text = tokenizer.texts_to_sequences(df['text_combined'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure they are the same length\n",
    "max_length = 100  # Set max length of sequences\n",
    "X_text = pad_sequences(X_text, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Separate the numerical features\n",
    "numerical_features = df.select_dtypes(['int64', 'int32', 'float64']).columns.tolist()\n",
    "numerical_features.remove('majority_target')\n",
    "X_num = df[numerical_features].values\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Combine text sequences and numerical features into one dataset\n",
    "X_combined = np.hstack((X_text, X_num_scaled))\n",
    "\n",
    "# Define the target variable\n",
    "y = df['majority_target'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=128, input_length=max_length))  # Embedding layer for text\n",
    "model.add(LSTM(128, return_sequences=True))  # LSTM layer\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(LSTM(64, return_sequences=False))  # Another LSTM layer\n",
    "model.add(Dropout(0.5))  # More regularization\n",
    "model.add(Dense(32, activation='relu'))  # Dense layer for additional complexity\n",
    "model.add(Dropout(0.5))  # Additional Dropout\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 243ms/step - accuracy: 0.5093 - loss: 0.6939 - val_accuracy: 0.5337 - val_loss: 0.6891\n",
      "Epoch 2/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 257ms/step - accuracy: 0.5325 - loss: 0.6891 - val_accuracy: 0.5274 - val_loss: 0.6905\n",
      "Epoch 3/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 258ms/step - accuracy: 0.5404 - loss: 0.6871 - val_accuracy: 0.9270 - val_loss: 0.2698\n",
      "Epoch 4/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 267ms/step - accuracy: 0.9482 - loss: 0.2253 - val_accuracy: 0.9532 - val_loss: 0.1889\n",
      "Epoch 5/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1940s\u001b[0m 826ms/step - accuracy: 0.9484 - loss: 0.2227 - val_accuracy: 0.9539 - val_loss: 0.1898\n",
      "Epoch 6/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 250ms/step - accuracy: 0.9563 - loss: 0.1994 - val_accuracy: 0.9557 - val_loss: 0.1829\n",
      "Epoch 7/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 239ms/step - accuracy: 0.9557 - loss: 0.2004 - val_accuracy: 0.9557 - val_loss: 0.1848\n",
      "Epoch 8/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 294ms/step - accuracy: 0.9560 - loss: 0.2024 - val_accuracy: 0.9453 - val_loss: 0.2154\n",
      "Epoch 9/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 243ms/step - accuracy: 0.9524 - loss: 0.2083 - val_accuracy: 0.9538 - val_loss: 0.1875\n",
      "Epoch 10/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1213s\u001b[0m 516ms/step - accuracy: 0.9563 - loss: 0.1979 - val_accuracy: 0.9539 - val_loss: 0.1874\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 61ms/step - accuracy: 0.9557 - loss: 0.1810\n",
      "Test Accuracy: 0.9558619260787964\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 55ms/step\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Accuracy: 0.9559\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.95      0.95      0.95     19442\n",
      "        Real       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18555   887]\n",
      " [  890 19928]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_use = (y_pred > 0.5).astype(int)\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_use)\n",
    "print(f\"RNN Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_use, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix to understand predictions\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e024445a82504603b5fad4c9b11e3b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laiye\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97644a2775f844a6bdd06d94551f1d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7883d719f744b3bb309efed33ea779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b0bec7a7c5460195531fe746c5adb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a43e868d53400fa0e09de82f006c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7446e4345dde496eaab3d41661ccbef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2503e78e57ca4d3c9b271830c874eec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5a60d66eed4ee79f7ec7e7b9445288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef53bdd6232425e88fc114534cab637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe811f5906408399436f762957c27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc25bc444f54ed6b7e9663f36318422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained sentence transformer model\n",
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate sentence embeddings for the 'statement' column\n",
    "X_statement_embeddings = model_sbert.encode(df['statement'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# Generate sentence embeddings for the 'tweet' column\n",
    "X_tweet_embeddings = model_sbert.encode(df['tweet'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "numerical_features = df.select_dtypes(['int64', 'int32', 'float64']).columns.tolist()\n",
    "numerical_features.remove('majority_target')\n",
    "X_num = df[numerical_features].values\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for statement, tweet, and numerical features\n",
    "input_statement = Input(shape=(X_statement_embeddings.shape[1],))\n",
    "input_tweet = Input(shape=(X_tweet_embeddings.shape[1],))\n",
    "input_numerical = Input(shape=(X_num_scaled.shape[1],))\n",
    "\n",
    "# Reshape to 3D for LSTM (adding a dimension for timesteps)\n",
    "reshaped_statement = Reshape((1, X_statement_embeddings.shape[1]))(input_statement)  # Shape: (batch_size, 1, 384)\n",
    "reshaped_tweet = Reshape((1, X_tweet_embeddings.shape[1]))(input_tweet)  # Shape: (batch_size, 1, 384)\n",
    "\n",
    "# LSTM for statement embeddings\n",
    "lstm_statement = LSTM(128, return_sequences=False)(reshaped_statement)\n",
    "\n",
    "# LSTM for tweet embeddings\n",
    "lstm_tweet = LSTM(128, return_sequences=False)(reshaped_tweet)\n",
    "\n",
    "# Combine LSTM outputs and numerical features\n",
    "combined = Concatenate()([lstm_statement, lstm_tweet, input_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layers\n",
    "# x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "x = Dense(64, activation='relu')(combined)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_statement, input_tweet, input_numerical], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_9', 'keras_tensor_10', 'keras_tensor_11']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8859 - loss: 0.3188 - val_accuracy: 0.9558 - val_loss: 0.1849 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9564 - loss: 0.2034 - val_accuracy: 0.9559 - val_loss: 0.1839 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9577 - loss: 0.1965 - val_accuracy: 0.9558 - val_loss: 0.1841 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9584 - loss: 0.1927 - val_accuracy: 0.9557 - val_loss: 0.1841 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9570 - loss: 0.1926 - val_accuracy: 0.9558 - val_loss: 0.1833 - learning_rate: 2.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9574 - loss: 0.1884 - val_accuracy: 0.9558 - val_loss: 0.1840 - learning_rate: 2.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9568 - loss: 0.1903 - val_accuracy: 0.9558 - val_loss: 0.1839 - learning_rate: 2.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9573 - loss: 0.1849 - val_accuracy: 0.9558 - val_loss: 0.1847 - learning_rate: 4.0000e-05\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9575 - loss: 0.1794\n",
      "Test Accuracy: 0.9576005935668945\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_statement, X_test_statement, X_train_tweet, X_test_tweet, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    X_statement_embeddings, X_tweet_embeddings, X_num_scaled, df['majority_target'], test_size=0.3, random_state=42)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_statement, X_test_tweet, X_test_num], y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([X_test_statement, X_test_tweet, X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Accuracy: 0.9576\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.95      0.96      0.96     19442\n",
      "        Real       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18618   824]\n",
      " [  883 19935]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_use = (y_pred > 0.5).astype(int)\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_use)\n",
    "print(f\"RNN Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_use, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix to understand predictions\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirection RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for statement, tweet, and numerical features\n",
    "input_statement = Input(shape=(X_statement_embeddings.shape[1],))\n",
    "input_tweet = Input(shape=(X_tweet_embeddings.shape[1],))\n",
    "input_numerical = Input(shape=(X_num_scaled.shape[1],))\n",
    "\n",
    "# Reshape to 3D for LSTM (adding a dimension for timesteps)\n",
    "reshaped_statement = Reshape((1, X_statement_embeddings.shape[1]))(input_statement)  # Shape: (batch_size, 1, 384)\n",
    "reshaped_tweet = Reshape((1, X_tweet_embeddings.shape[1]))(input_tweet)  # Shape: (batch_size, 1, 384)\n",
    "\n",
    "# Bidirectional LSTM for statement embeddings\n",
    "bidirectional_lstm_statement = Bidirectional(LSTM(128, return_sequences=False))(reshaped_statement)\n",
    "\n",
    "# Bidirectional LSTM for tweet embeddings\n",
    "bidirectional_lstm_tweet = Bidirectional(LSTM(128, return_sequences=False))(reshaped_tweet)\n",
    "\n",
    "# Combine LSTM outputs and numerical features\n",
    "combined = Concatenate()([bidirectional_lstm_statement, bidirectional_lstm_tweet, input_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layers\n",
    "# x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "x = Dense(64, activation='relu')(combined)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_statement, input_tweet, input_numerical], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_22', 'keras_tensor_23', 'keras_tensor_24']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.8956 - loss: 0.3037 - val_accuracy: 0.9557 - val_loss: 0.1846 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.9564 - loss: 0.2034 - val_accuracy: 0.9558 - val_loss: 0.1852 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9565 - loss: 0.2000 - val_accuracy: 0.9558 - val_loss: 0.1834 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9556 - loss: 0.1995 - val_accuracy: 0.9558 - val_loss: 0.1831 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.9572 - loss: 0.1934 - val_accuracy: 0.9558 - val_loss: 0.1839 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.9579 - loss: 0.1878 - val_accuracy: 0.9558 - val_loss: 0.1840 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9560 - loss: 0.1892 - val_accuracy: 0.9558 - val_loss: 0.1868 - learning_rate: 2.0000e-04\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9575 - loss: 0.1819\n",
      "Test Accuracy: 0.9576005935668945\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_statement, X_test_statement, X_train_tweet, X_test_tweet, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    X_statement_embeddings, X_tweet_embeddings, X_num_scaled, df['majority_target'], test_size=0.3, random_state=42)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_statement, X_test_tweet, X_test_num], y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([X_test_statement, X_test_tweet, X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9573 - loss: 0.1840 - val_accuracy: 0.9558 - val_loss: 0.1878 - learning_rate: 2.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.9572 - loss: 0.1798 - val_accuracy: 0.9558 - val_loss: 0.1872 - learning_rate: 2.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.9586 - loss: 0.1757 - val_accuracy: 0.9558 - val_loss: 0.1889 - learning_rate: 2.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9577 - loss: 0.1782 - val_accuracy: 0.9558 - val_loss: 0.1892 - learning_rate: 2.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.9571 - loss: 0.1788 - val_accuracy: 0.9558 - val_loss: 0.1891 - learning_rate: 4.0000e-05\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1847\n",
      "Test Accuracy: 0.9576005935668945\n",
      "\u001b[1m1259/1259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_statement, X_test_statement, X_train_tweet, X_test_tweet, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    X_statement_embeddings, X_tweet_embeddings, X_num_scaled, df['majority_target'], test_size=0.3, random_state=42)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_statement, X_test_tweet, X_test_num], y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([X_test_statement, X_test_tweet, X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Accuracy: 0.9576\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.95      0.96      0.96     19442\n",
      "        Real       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18618   824]\n",
      " [  883 19935]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_use = (y_pred > 0.5).astype(int)\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_use)\n",
    "print(f\"RNN Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_use, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix to understand predictions\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic tokenizer gives 0.9559\n",
    "\n",
    "with sentence embeddings:\n",
    "\n",
    "- the one with regularizer gives 0.9571 accuracy\n",
    "- basic with only dropout is 0.9576\n",
    "- with BatchNormalization is 0.9576\n",
    "- with reduce learning rate and early stopping is also 0.9576\n",
    "- Bi-directional RNN is 0.9576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_35', 'keras_tensor_36', 'keras_tensor_37']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_48', 'keras_tensor_49', 'keras_tensor_50']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_61', 'keras_tensor_62', 'keras_tensor_63']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_74', 'keras_tensor_75', 'keras_tensor_76']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laiye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_87', 'keras_tensor_88', 'keras_tensor_89']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Cross-Validation Results (5 folds):\n",
      "Accuracy: 0.9572 ± 0.0006\n",
      "Precision: 0.9588 ± 0.0021\n",
      "Recall: 0.9580 ± 0.0013\n",
      "F1 Score: 0.9584 ± 0.0006\n",
      "AUC: 0.9585 ± 0.0002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list, precision_list, recall_list, f1_list, auc_list = [], [], [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(X_statement_embeddings):\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train_statement, X_val_statement = X_statement_embeddings[train_index], X_statement_embeddings[val_index]\n",
    "    X_train_tweet, X_val_tweet = X_tweet_embeddings[train_index], X_tweet_embeddings[val_index]\n",
    "    X_train_num, X_val_num = X_num_scaled[train_index], X_num_scaled[val_index]\n",
    "    y_train, y_val = df['majority_target'].iloc[train_index], df['majority_target'].iloc[val_index]\n",
    "    \n",
    "    # Define inputs and model\n",
    "    input_statement = Input(shape=(X_statement_embeddings.shape[1],))\n",
    "    input_tweet = Input(shape=(X_tweet_embeddings.shape[1],))\n",
    "    input_numerical = Input(shape=(X_num_scaled.shape[1],))\n",
    "    \n",
    "    # Reshape to 3D for LSTM (adding a dimension for timesteps)\n",
    "    reshaped_statement = Reshape((1, X_statement_embeddings.shape[1]))(input_statement)\n",
    "    reshaped_tweet = Reshape((1, X_tweet_embeddings.shape[1]))(input_tweet)\n",
    "    \n",
    "    # LSTM for statement and tweet embeddings\n",
    "    lstm_statement = LSTM(128, return_sequences=False)(reshaped_statement)\n",
    "    lstm_tweet = LSTM(128, return_sequences=False)(reshaped_tweet)\n",
    "    \n",
    "    # Combine LSTM outputs and numerical features\n",
    "    combined = Concatenate()([lstm_statement, lstm_tweet, input_numerical])\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Define and compile model\n",
    "    model = Model(inputs=[input_statement, input_tweet, input_numerical], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit([X_train_statement, X_train_tweet, X_train_num], y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    y_pred_prob = model.predict([X_val_statement, X_val_tweet, X_val_num])\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred_prob)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "# Display the results for each metric\n",
    "print(f\"Cross-Validation Results ({n_folds} folds):\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_list):.4f} ± {np.std(accuracy_list):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_list):.4f} ± {np.std(precision_list):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_list):.4f} ± {np.std(recall_list):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Robustness: The model is robust and performs consistently well across different data splits, showing it has generalized well to the dataset.\n",
    "\n",
    "Performance Ceiling: Since the accuracy, F1 score, and AUC are all very close to 0.96, it’s likely that the model reached a performance ceiling with this dataset and feature set. Additional tuning may not yield significant improvements because the model has effectively captured the key patterns for distinguishing fake from real news.\n",
    "\n",
    "Balanced Metrics: The high scores in precision and recall, along with a closely matching F1 score, indicate that the model maintains a good balance between identifying both real and fake news accurately, with a strong AUC showing good separation between classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
