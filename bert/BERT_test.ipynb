{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnG-sTCJe5rW",
        "outputId": "b9f350bc-c352-42c4-e276-f0c5dd8f0184"
      },
      "outputs": [],
      "source": [
        "!! pip install --upgrade transformers\n",
        "!! pip install tf-keras\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0tKeyl9wbBAD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "otoDFyaVbBAF"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Features_For_Traditional_ML_Techniques.csv\", index_col=0)\n",
        "subset_data = df.sample(frac=0.1, random_state=42)\n",
        "texts = df['tweet'].values\n",
        "labels = df['majority_target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "134198"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lx2PTbjsbBAF"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(texts, labels, tokenizer, batch_size=32, max_length=64):\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors='tf',\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    # Create optimized dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask']\n",
        "        },\n",
        "        labels\n",
        "    ))\n",
        "\n",
        "    # Optimize performance\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhWCXp_bBAF",
        "outputId": "629c0e3a-a3fc-456c-f9e5-64e98377b192"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=1\n",
        ")\n",
        "\n",
        "# Corrected optimizer variable\n",
        "optimizer = Adam(learning_rate=2e-5)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer= optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvGLuBsebBAG"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OzOYF80kbBAG"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    subset_data['tweet'].values,\n",
        "    subset_data['majority_target'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "X_train, y_train, test_size = 0.2, random_state=42)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare datasets with optimized parameters\n",
        "train_dataset = prepare_dataset(X_train, y_train, tokenizer)\n",
        "val_dataset = prepare_dataset(X_val, y_val, tokenizer)\n",
        "test_dataset = prepare_dataset(X_test, y_test, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pUQsa0abBAG",
        "outputId": "cfa3baa7-de24-44f8-ff60-5e1e1a44bba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "269/269 [==============================] - 1951s 7s/step - loss: 0.6914 - accuracy: 0.6772 - val_loss: 0.5523 - val_accuracy: 0.7561 - lr: 2.0000e-05\n",
            "Epoch 2/8\n",
            "269/269 [==============================] - 1499s 6s/step - loss: 0.5305 - accuracy: 0.8040 - val_loss: 0.4586 - val_accuracy: 0.8613 - lr: 2.0000e-05\n",
            "Epoch 3/8\n",
            "269/269 [==============================] - 1659s 6s/step - loss: 0.6137 - accuracy: 0.7266 - val_loss: 0.5537 - val_accuracy: 0.7244 - lr: 2.0000e-05\n",
            "Epoch 4/8\n",
            "269/269 [==============================] - 1832s 7s/step - loss: 0.5132 - accuracy: 0.8259 - val_loss: 0.6409 - val_accuracy: 0.6993 - lr: 2.0000e-05\n",
            "Epoch 5/8\n",
            "269/269 [==============================] - 2020s 8s/step - loss: 0.4665 - accuracy: 0.8911 - val_loss: 0.6668 - val_accuracy: 0.8729 - lr: 2.0000e-05\n",
            "Epoch 6/8\n",
            "269/269 [==============================] - 1443s 5s/step - loss: 0.3967 - accuracy: 0.9165 - val_loss: 0.5557 - val_accuracy: 0.8934 - lr: 4.0000e-06\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=4,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=8,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Qno0vDXXbBAH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 129s 2s/step - loss: 0.4675 - accuracy: 0.8636\n",
            "Test Loss: 0.4675, Test Accuracy: 0.8636\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 127s 1s/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of logits: (2684, 1)\n"
          ]
        }
      ],
      "source": [
        "logits = y_pred.logits  # This may vary based on your model type\n",
        "\n",
        "# Check the shape of logits\n",
        "print(\"Shape of logits:\", logits.shape)\n",
        "\n",
        "# Determine class labels based on the output shape\n",
        "if len(logits.shape) == 1:  # Binary classification\n",
        "    y_pred_classes = (logits > 0.5).astype(int).flatten()\n",
        "else:  # Multiclass classification\n",
        "    y_pred_classes = np.argmax(logits, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.4675\n",
            "Test Accuracy: 0.8636\n",
            "Precision: 0.2411\n",
            "Recall: 0.4911\n",
            "F1 Score: 0.3234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "report = classification_report(y_test, y_pred_classes, output_dict=True)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "print(f'Precision: {report[\"weighted avg\"][\"precision\"]:.4f}')\n",
        "print(f'Recall: {report[\"weighted avg\"][\"recall\"]:.4f}')\n",
        "print(f'F1 Score: {report[\"weighted avg\"][\"f1-score\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bert_model_whole_dataset\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bert_model_whole_dataset\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save('bert_model_whole_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained('bert_model_huggingface_whole_dataset')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
