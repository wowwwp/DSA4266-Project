{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnG-sTCJe5rW",
        "outputId": "b9f350bc-c352-42c4-e276-f0c5dd8f0184"
      },
      "outputs": [],
      "source": [
        "!! pip install --upgrade transformers\n",
        "!! pip install tf-keras\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0tKeyl9wbBAD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "otoDFyaVbBAF"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Features_For_Traditional_ML_Techniques.csv\", index_col=0)\n",
        "subset_data = df.sample(frac=0.1, random_state=42)\n",
        "texts = df['tweet'].values\n",
        "labels = df['majority_target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "134198"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lx2PTbjsbBAF"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(texts, labels, tokenizer, batch_size=32, max_length=64):\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors='tf',\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    # Create optimized dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask']\n",
        "        },\n",
        "        labels\n",
        "    ))\n",
        "\n",
        "    # Optimize performance\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhWCXp_bBAF",
        "outputId": "629c0e3a-a3fc-456c-f9e5-64e98377b192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=1\n",
        ")\n",
        "\n",
        "# Corrected optimizer variable\n",
        "optimizer = Adam(learning_rate=2e-5)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer= optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvGLuBsebBAG"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OzOYF80kbBAG"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    subset_data['tweet'].values,\n",
        "    subset_data['majority_target'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare datasets with optimized parameters\n",
        "train_dataset = prepare_dataset(X_train, y_train, tokenizer)\n",
        "test_dataset = prepare_dataset(X_test, y_test, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pUQsa0abBAG",
        "outputId": "cfa3baa7-de24-44f8-ff60-5e1e1a44bba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "WARNING:tensorflow:From c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "336/336 [==============================] - 1676s 5s/step - loss: 0.6429 - accuracy: 0.6873 - val_loss: 0.4133 - val_accuracy: 0.8554 - lr: 2.0000e-05\n",
            "Epoch 2/8\n",
            "260/336 [======================>.......] - ETA: 8:16 - loss: 0.4642 - accuracy: 0.8465"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=4,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=8,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qno0vDXXbBAH"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of logits: (2684, 1)\n"
          ]
        }
      ],
      "source": [
        "logits = y_pred.logits  # This may vary based on your model type\n",
        "\n",
        "# Check the shape of logits\n",
        "print(\"Shape of logits:\", logits.shape)\n",
        "\n",
        "# Determine class labels based on the output shape\n",
        "if len(logits.shape) == 1:  # Binary classification\n",
        "    y_pred_classes = (logits > 0.5).astype(int).flatten()\n",
        "else:  # Multiclass classification\n",
        "    y_pred_classes = np.argmax(logits, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.5524\n",
            "Test Accuracy: 0.8875\n",
            "Precision: 0.2411\n",
            "Recall: 0.4911\n",
            "F1 Score: 0.3234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\madel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "report = classification_report(y_test, y_pred_classes, output_dict=True)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "print(f'Precision: {report[\"weighted avg\"][\"precision\"]:.4f}')\n",
        "print(f'Recall: {report[\"weighted avg\"][\"recall\"]:.4f}')\n",
        "print(f'F1 Score: {report[\"weighted avg\"][\"f1-score\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bert_model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bert_model\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save('bert_model_whole_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained('bert_model_huggingface_whole_dataset')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
