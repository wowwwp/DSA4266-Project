{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf94c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6710/6710 [==============================] - 271s 39ms/step - loss: 0.4631 - accuracy: 0.8016 - val_loss: 0.3320 - val_accuracy: 0.8754 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "6710/6710 [==============================] - 259s 39ms/step - loss: 0.3333 - accuracy: 0.8869 - val_loss: 0.2803 - val_accuracy: 0.9052 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "6710/6710 [==============================] - 248s 37ms/step - loss: 0.2962 - accuracy: 0.9062 - val_loss: 0.2925 - val_accuracy: 0.8984 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "6710/6710 [==============================] - 248s 37ms/step - loss: 0.2725 - accuracy: 0.9182 - val_loss: 0.2626 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "6710/6710 [==============================] - 254s 38ms/step - loss: 0.2572 - accuracy: 0.9251 - val_loss: 0.2447 - val_accuracy: 0.9251 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "6710/6710 [==============================] - 247s 37ms/step - loss: 0.2462 - accuracy: 0.9302 - val_loss: 0.2420 - val_accuracy: 0.9256 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "6710/6710 [==============================] - 251s 37ms/step - loss: 0.2373 - accuracy: 0.9346 - val_loss: 0.2443 - val_accuracy: 0.9262 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "6710/6710 [==============================] - 251s 37ms/step - loss: 0.2310 - accuracy: 0.9369 - val_loss: 0.2403 - val_accuracy: 0.9263 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "6710/6710 [==============================] - 361s 54ms/step - loss: 0.2244 - accuracy: 0.9403 - val_loss: 0.2366 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "6710/6710 [==============================] - 581s 87ms/step - loss: 0.2214 - accuracy: 0.9415 - val_loss: 0.2393 - val_accuracy: 0.9272 - lr: 0.0010\n",
      "839/839 [==============================] - 27s 32ms/step - loss: 0.2393 - accuracy: 0.9272\n",
      "Test Accuracy: 0.9272\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Load pre-trained GloVe model (from Gensim API)\n",
    "word2vec_model = api.load(\"glove-wiki-gigaword-50\")  # Automatically downloads 50-dimensional vectors\n",
    "\n",
    "# Step 2: Load your dataset\n",
    "df = pd.read_csv(\"Features_For_Traditional_ML_Techniques.csv\")\n",
    "df = df.drop('Unnamed: 0', axis=1)  # Drop unnecessary index column\n",
    "\n",
    "# Convert 'majority_target' to integers (0 for false, 1 for true)\n",
    "df['majority_target'] = df['majority_target'].astype(int)\n",
    "\n",
    "# Define features (X) and labels (y)\n",
    "X = df['tweet']\n",
    "y = df['majority_target']\n",
    "\n",
    "# Step 3: Tokenize and pad the sequences\n",
    "max_words = 10000  # Maximum number of words to keep in the vocabulary\n",
    "max_seq_length = 50  # Reduced maximum sequence length for padding\n",
    "\n",
    "# Tokenizer to convert tweets to sequences of integers\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Prepare the embedding matrix using GloVe\n",
    "embedding_dim = 50  # GloVe model's dimensionality\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Initialize the embedding matrix with zeros\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "# Populate the embedding matrix with GloVe vectors for words in the dataset\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    else:\n",
    "        # Randomly initialize words not found in GloVe model\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "# Step 5: Define the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the embedding layer (with pre-trained GloVe embeddings)\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "\n",
    "# Step 6: Add the LSTM layers and dropout\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Step 7: Add a fully connected dense layer with ReLU\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Step 8: Add the output layer (for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Step 9: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 10: Implement EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "# Step 11: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 12: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16,\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Step 13: Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbf42c",
   "metadata": {},
   "source": [
    "Test Accuracy: 0.9272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72af76bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6710/6710 [==============================] - 423s 62ms/step - loss: 0.4733 - accuracy: 0.7909 - val_loss: 0.3616 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "6710/6710 [==============================] - 398s 59ms/step - loss: 0.3624 - accuracy: 0.8683 - val_loss: 0.2956 - val_accuracy: 0.8974 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "6710/6710 [==============================] - 394s 59ms/step - loss: 0.3225 - accuracy: 0.8909 - val_loss: 0.2873 - val_accuracy: 0.8994 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "6710/6710 [==============================] - 413s 62ms/step - loss: 0.3036 - accuracy: 0.9005 - val_loss: 0.2617 - val_accuracy: 0.9139 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "6710/6710 [==============================] - 473s 71ms/step - loss: 0.2868 - accuracy: 0.9093 - val_loss: 0.2541 - val_accuracy: 0.9179 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "6710/6710 [==============================] - 482s 72ms/step - loss: 0.2806 - accuracy: 0.9122 - val_loss: 0.2502 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "6710/6710 [==============================] - 491s 73ms/step - loss: 0.2748 - accuracy: 0.9158 - val_loss: 0.2459 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "6710/6710 [==============================] - 488s 73ms/step - loss: 0.2674 - accuracy: 0.9187 - val_loss: 0.2437 - val_accuracy: 0.9251 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "6710/6710 [==============================] - 479s 71ms/step - loss: 0.2657 - accuracy: 0.9198 - val_loss: 0.2438 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "6710/6710 [==============================] - 483s 72ms/step - loss: 0.2599 - accuracy: 0.9215 - val_loss: 0.2356 - val_accuracy: 0.9280 - lr: 0.0010\n",
      "839/839 [==============================] - 22s 26ms/step - loss: 0.2356 - accuracy: 0.9280\n",
      "Test Accuracy: 0.9280\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_csv(\"Features_For_Traditional_ML_Techniques.csv\")  # Replace with your file path\n",
    "tweets = df['tweet'].values\n",
    "y = df['majority_target'].values  # Replace with the actual target column\n",
    "\n",
    "# Step 2: Tokenize the 'tweet' column\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit the vocabulary to the 5000 most frequent words\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "word_index = tokenizer.word_index  # Dictionary mapping words to their indices\n",
    "\n",
    "# Step 3: Pad sequences to ensure uniform input length\n",
    "max_len = 100  # You can adjust this based on your dataset\n",
    "X_padded = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Step 4: Train a FastText model on your dataset\n",
    "# FastText learns subword information, making it robust for rare and unseen words\n",
    "fasttext_model = FastText(sentences=[tweet.split() for tweet in tweets], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 5: Create the embedding matrix from FastText model\n",
    "embedding_dim = 100  # FastText embedding dimension\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "# Step 6: Define the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the embedding layer (with FastText embeddings)\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=False))  # Freeze the embedding layer\n",
    "\n",
    "# Step 7: Add the LSTM layers and dropout\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Step 8: Add a fully connected dense layer with ReLU\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Step 9: Add the output layer (for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Step 10: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 11: Implement EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "# Step 12: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 13: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16,\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Step 14: Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a750aaf",
   "metadata": {},
   "source": [
    "Test Accuracy: 0.9280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0417d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6710/6710 [==============================] - 663s 94ms/step - loss: 0.4299 - accuracy: 0.8244 - val_loss: 0.3288 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "6710/6710 [==============================] - 345s 51ms/step - loss: 0.3080 - accuracy: 0.9005 - val_loss: 0.2603 - val_accuracy: 0.9155 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "6710/6710 [==============================] - 237s 35ms/step - loss: 0.2662 - accuracy: 0.9217 - val_loss: 0.2411 - val_accuracy: 0.9254 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "6710/6710 [==============================] - 251s 37ms/step - loss: 0.2467 - accuracy: 0.9305 - val_loss: 0.2345 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "6710/6710 [==============================] - 298s 44ms/step - loss: 0.2324 - accuracy: 0.9369 - val_loss: 0.2327 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "6710/6710 [==============================] - 298s 44ms/step - loss: 0.2232 - accuracy: 0.9420 - val_loss: 0.2342 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "6710/6710 [==============================] - 297s 44ms/step - loss: 0.2156 - accuracy: 0.9445 - val_loss: 0.2410 - val_accuracy: 0.9270 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "6710/6710 [==============================] - 298s 44ms/step - loss: 0.1966 - accuracy: 0.9520 - val_loss: 0.2285 - val_accuracy: 0.9337 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "6710/6710 [==============================] - 297s 44ms/step - loss: 0.1891 - accuracy: 0.9554 - val_loss: 0.2302 - val_accuracy: 0.9334 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "6710/6710 [==============================] - 296s 44ms/step - loss: 0.1825 - accuracy: 0.9573 - val_loss: 0.2378 - val_accuracy: 0.9322 - lr: 2.0000e-04\n",
      "839/839 [==============================] - 21s 25ms/step - loss: 0.2378 - accuracy: 0.9322\n",
      "Test Accuracy: 0.9322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Load pre-trained Word2Vec model (from Gensim API)\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # Automatically downloads 300-dimensional vectors\n",
    "\n",
    "# Step 2: Load your dataset\n",
    "df = pd.read_csv(\"Features_For_Traditional_ML_Techniques.csv\")\n",
    "df = df.drop('Unnamed: 0', axis=1)  # Drop unnecessary index column\n",
    "\n",
    "# Convert 'majority_target' to integers (0 for false, 1 for true)\n",
    "df['majority_target'] = df['majority_target'].astype(int)\n",
    "\n",
    "# Define features (X) and labels (y)\n",
    "X = df['tweet']\n",
    "y = df['majority_target']\n",
    "\n",
    "# Step 3: Tokenize and pad the sequences\n",
    "max_words = 10000  # Maximum number of words to keep in the vocabulary\n",
    "max_seq_length = 50  # Reduced maximum sequence length for padding\n",
    "\n",
    "# Tokenizer to convert tweets to sequences of integers\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Prepare the embedding matrix using Word2Vec\n",
    "embedding_dim = 300  # Word2Vec model's dimensionality\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Initialize the embedding matrix with zeros\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "# Populate the embedding matrix with Word2Vec vectors for words in the dataset\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    else:\n",
    "        # Randomly initialize words not found in Word2Vec model\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "# Step 5: Define the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the embedding layer (with pre-trained Word2Vec embeddings)\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "\n",
    "# Step 6: Add the LSTM layers and dropout\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Step 7: Add a fully connected dense layer with ReLU\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Step 8: Add the output layer (for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Step 9: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 10: Implement EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "# Step 11: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 12: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16,\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Step 13: Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1fc82",
   "metadata": {},
   "source": [
    "Test Accuracy: 0.9322"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
