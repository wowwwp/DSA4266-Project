{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584fb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9407352210630899\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.93      0.94     19442\n",
      "        True       0.94      0.95      0.94     20818\n",
      "\n",
      "    accuracy                           0.94     40260\n",
      "   macro avg       0.94      0.94      0.94     40260\n",
      "weighted avg       0.94      0.94      0.94     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Features_For_Traditional_ML_Techniques.csv')\n",
    "\n",
    "# Extract the tweet text and labels\n",
    "tweets = df['tweet'].astype(str).values  # Using 'tweet' as the text input\n",
    "labels = df['majority_target'].values  # Assuming this is the target label\n",
    "\n",
    "# Create a TF-IDF vectorizer with a limited vocabulary size\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 words\n",
    "tfidf_features = vectorizer.fit_transform(tweets)  # Keep this as a sparse matrix\n",
    "\n",
    "# Apply TruncatedSVD for dimensionality reduction on TF-IDF features\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)  # Reduce to 100 components\n",
    "tfidf_reduced = svd.fit_transform(tfidf_features)\n",
    "\n",
    "# Combine TF-IDF reduced features with the rest of your features\n",
    "extra_features = df.drop(columns=['tweet', 'statement', 'majority_target', 'BinaryNumTarget'])  # Drop non-feature columns\n",
    "\n",
    "# Ensure all extra features are numeric, convert or fill NaNs\n",
    "extra_features = extra_features.apply(pd.to_numeric, errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "extra_features.fillna(0, inplace=True)  # Fill NaN values with 0 (or another strategy)\n",
    "\n",
    "# Convert extra features to a sparse matrix\n",
    "extra_features_sparse = csr_matrix(extra_features.values)\n",
    "\n",
    "# Combine the extra sparse features with the reduced TF-IDF features\n",
    "combined_features = hstack([extra_features_sparse, csr_matrix(tfidf_reduced)])  # Combine both as sparse matrices\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Classifier with optimizations\n",
    "rf_model = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)  # 50 trees and use all CPU cores\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73f4f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9576\n",
      "\n",
      "Classification Report: training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.96      0.96      0.96     45771\n",
      "        Real       0.96      0.96      0.96     48167\n",
      "\n",
      "    accuracy                           0.96     93938\n",
      "   macro avg       0.96      0.96      0.96     93938\n",
      "weighted avg       0.96      0.96      0.96     93938\n",
      "\n",
      "\n",
      "Classification Report: test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.95      0.96      0.96     19442\n",
      "        Real       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix# Load the dataset\n",
    "df = pd.read_csv('Features_For_Traditional_ML_Techniques.csv')\n",
    "# Initialize the lemmatizer and stopwords\n",
    "df = df.drop('Unnamed: 0', axis=1) #dropping this column\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess a single tweet\n",
    "def preprocess_tweet(tweet):\n",
    "    # 1. Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # 2. Remove URLs, mentions (@username), hashtags, and special characters\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\w+|\\#', '', tweet)  # Remove mentions and hashtags\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove punctuation and special characters\n",
    "    \n",
    "    # 3. Tokenization (split the tweet into words)\n",
    "    words = tweet.split()\n",
    "    \n",
    "    # 4. Remove stopwords and apply lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing to the 'tweet' column\n",
    "df['processed_tweet'] = df['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "\n",
    "\n",
    "def replace_spaces_with_commas(emb_str):\n",
    "    return emb_str.replace(' ', ',')\n",
    "\n",
    "# Apply the function to the embeddings column\n",
    "df['embeddings'] = df['embeddings'].apply(replace_spaces_with_commas)\n",
    "\n",
    "# Function to extract the list from the string representation\n",
    "def extract_list(emb_str):\n",
    "    # Remove outer brackets and convert the string to a list\n",
    "    return eval(emb_str.strip(\"[]\"))\n",
    "\n",
    "# Apply the function and create a DataFrame from the list of embeddings\n",
    "embeddings_split = df['embeddings'].apply(extract_list).apply(pd.Series)\n",
    "\n",
    "# Rename columns to reflect the split embedding positions\n",
    "embeddings_split.columns = [f'embedding_{i}' for i in range(embeddings_split.shape[1])]\n",
    "\n",
    "# Concatenate the original DataFrame with the new embeddings DataFrame\n",
    "df = pd.concat([df, embeddings_split], axis=1)\n",
    "df = df.drop(['embeddings'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the tweet text and labels\n",
    "tweets = df['processed_tweet'].astype(str).values  # Using 'tweet' as the text input\n",
    "y = df['majority_target']\n",
    "\n",
    "# Create a TF-IDF vectorizer with a limited vocabulary size\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_features = vectorizer.fit_transform(tweets)\n",
    "\n",
    "# Apply TruncatedSVD for dimensionality reduction on TF-IDF features\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_features)\n",
    "\n",
    "# Now drop the 'majority_target', 'statement', and 'tweet' columns from df\n",
    "X_other = df.drop(['majority_target', 'statement', 'tweet'], axis=1)\n",
    "\n",
    "# Convert all non-numeric columns to numeric, or fill NaN with 0\n",
    "X_other = X_other.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "# Scale the numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_other_scaled = scaler.fit_transform(X_other)  # This will return a dense array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert X_other to a sparse matrix since we will combine it with the sparse TF-IDF matrix\n",
    "X_other_sparse = csr_matrix(X_other_scaled)\n",
    "\n",
    "# Combine the TF-IDF reduced features and the other features\n",
    "X_combined = hstack([X_other_sparse, csr_matrix(tfidf_reduced)])\n",
    "\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth= 10, random_state=42)\n",
    "\n",
    "# Train the Random Forest on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_train = rf.predict(X_train)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report: training set\")\n",
    "print(classification_report(y_train, y_pred_train, target_names=['Fake', 'Real']))\n",
    "\n",
    "print(\"\\nClassification Report: test set\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "306c5a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8160\n",
      "\n",
      "Classification Report: training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.91      0.70      0.79     45771\n",
      "        Real       0.77      0.93      0.84     48167\n",
      "\n",
      "    accuracy                           0.82     93938\n",
      "   macro avg       0.84      0.82      0.82     93938\n",
      "weighted avg       0.84      0.82      0.82     93938\n",
      "\n",
      "\n",
      "Classification Report: test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.90      0.70      0.79     19442\n",
      "        Real       0.77      0.93      0.84     20818\n",
      "\n",
      "    accuracy                           0.82     40260\n",
      "   macro avg       0.83      0.81      0.81     40260\n",
      "weighted avg       0.83      0.82      0.81     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix# Load the dataset\n",
    "df = pd.read_csv('Features_For_Traditional_ML_Techniques.csv')\n",
    "# Initialize the lemmatizer and stopwords\n",
    "df = df.drop('Unnamed: 0', axis=1) #dropping this column\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Function to preprocess a single tweet\n",
    "# def preprocess_tweet(tweet):\n",
    "#     # 1. Convert to lowercase\n",
    "#     tweet = tweet.lower()\n",
    "    \n",
    "#     # 2. Remove URLs, mentions (@username), hashtags, and special characters\n",
    "#     tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet)  # Remove URLs\n",
    "#     tweet = re.sub(r'@\\w+|\\#', '', tweet)  # Remove mentions and hashtags\n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove punctuation and special characters\n",
    "    \n",
    "#     # 3. Tokenization (split the tweet into words)\n",
    "#     words = tweet.split()\n",
    "    \n",
    "#     # 4. Remove stopwords and apply lemmatization\n",
    "#     words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "#     # Join the words back into a single string\n",
    "#     return ' '.join(words)\n",
    "\n",
    "# # Apply the preprocessing to the 'tweet' column\n",
    "# df['processed_tweet'] = df['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "\n",
    "# Extract the tweet text and labels\n",
    "tweets = df['tweet'].astype(str).values  # Using 'tweet' as the text input\n",
    "y = df['majority_target']\n",
    "\n",
    "# Create a TF-IDF vectorizer with a limited vocabulary size\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_features = vectorizer.fit_transform(tweets)\n",
    "\n",
    "# # Apply TruncatedSVD for dimensionality reduction on TF-IDF features\n",
    "# svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "# tfidf_reduced = svd.fit_transform(tfidf_features)\n",
    "\n",
    "# Combine the TF-IDF reduced features and the other features\n",
    "X_combined = hstack([tfidf_features])\n",
    "\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth= 10, random_state=42)\n",
    "\n",
    "# Train the Random Forest on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_train = rf.predict(X_train)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report: training set\")\n",
    "print(classification_report(y_train, y_pred_train, target_names=['Fake', 'Real']))\n",
    "\n",
    "print(\"\\nClassification Report: test set\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfbc6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.957625434674615\n",
      "\n",
      "Classification Report: training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.96      0.96      0.96     45771\n",
      "        Real       0.97      0.97      0.97     48167\n",
      "\n",
      "    accuracy                           0.96     93938\n",
      "   macro avg       0.96      0.96      0.96     93938\n",
      "weighted avg       0.96      0.96      0.96     93938\n",
      "\n",
      "\n",
      "Classification Report: test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.95      0.96      0.96     19442\n",
      "        Real       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Features_For_Traditional_ML_Techniques.csv')\n",
    "\n",
    "# Drop the unnecessary column\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Function to replace spaces with commas in the 'embeddings' column\n",
    "def replace_spaces_with_commas(emb_str):\n",
    "    return emb_str.replace(' ', ',')\n",
    "\n",
    "# Apply the function to the embeddings column\n",
    "df['embeddings'] = df['embeddings'].apply(replace_spaces_with_commas)\n",
    "\n",
    "# Function to extract list from the string representation of embeddings\n",
    "def extract_list(emb_str):\n",
    "    return eval(emb_str.strip(\"[]\"))\n",
    "\n",
    "# Apply the function and create a DataFrame from the list of embeddings\n",
    "embeddings_split = df['embeddings'].apply(extract_list).apply(pd.Series)\n",
    "\n",
    "# Rename columns to reflect the split embedding positions\n",
    "embeddings_split.columns = [f'embedding_{i}' for i in range(embeddings_split.shape[1])]\n",
    "\n",
    "# Concatenate the original DataFrame with the new embeddings DataFrame\n",
    "df = pd.concat([df, embeddings_split], axis=1)\n",
    "\n",
    "# Drop the original embeddings column\n",
    "df = df.drop(['embeddings'], axis=1)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "y = df['majority_target']\n",
    "X = df.drop(['majority_target', 'statement', 'tweet'], axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)\n",
    "\n",
    "# Train the Random Forest on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_train = rf.predict(X_train)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.15f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report: training set\")\n",
    "print(classification_report(y_train, y_pred_train, target_names=['Fake', 'Real']))\n",
    "\n",
    "print(\"\\nClassification Report: test set\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede3d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best Parameters: {'bootstrap': True, 'max_depth': 38, 'min_samples_leaf': 15, 'min_samples_split': 12, 'n_estimators': 121}\n",
      "Accuracy: 0.957625434674615\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.96      0.96     19442\n",
      "        True       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Features_For_Traditional_ML_Techniques.csv')\n",
    "\n",
    "# Drop the unnecessary column\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Function to replace spaces with commas in the 'embeddings' column\n",
    "def replace_spaces_with_commas(emb_str):\n",
    "    return emb_str.replace(' ', ',')\n",
    "\n",
    "# Apply the function to the embeddings column\n",
    "df['embeddings'] = df['embeddings'].apply(replace_spaces_with_commas)\n",
    "\n",
    "# Function to extract list from the string representation of embeddings\n",
    "def extract_list(emb_str):\n",
    "    return eval(emb_str.strip(\"[]\"))\n",
    "\n",
    "# Apply the function and create a DataFrame from the list of embeddings\n",
    "embeddings_split = df['embeddings'].apply(extract_list).apply(pd.Series)\n",
    "\n",
    "# Rename columns to reflect the split embedding positions\n",
    "embeddings_split.columns = [f'embedding_{i}' for i in range(embeddings_split.shape[1])]\n",
    "\n",
    "# Concatenate the original DataFrame with the new embeddings DataFrame\n",
    "df = pd.concat([df, embeddings_split], axis=1)\n",
    "\n",
    "# Drop the original embeddings column\n",
    "df = df.drop(['embeddings'], axis=1)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "y = df['majority_target']\n",
    "X = df.drop(['majority_target', 'statement', 'tweet'], axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV using scipy's randint distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees\n",
    "    'max_depth': randint(10, 50),  # Maximum depth of the tree\n",
    "    'min_samples_split': randint(2, 20),  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': randint(1, 20),  # Minimum samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Reduced n_iter and cv for quicker execution\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, param_distributions=param_dist, n_iter=6, cv=2, \n",
    "    verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model with RandomizedSearchCV to find the best parameters\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator and print the best parameters\n",
    "best_rf_model = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e1cdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters for XGBoost: {'colsample_bytree': 0.6399899663272012, 'gamma': 0.22962444598293358, 'learning_rate': 0.11011258334170654, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 102, 'subsample': 0.9879639408647978}\n",
      "XGBoost Accuracy: 0.957625434674615\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.96      0.96     19442\n",
      "        True       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees\n",
    "    'max_depth': randint(3, 10),  # Maximum depth of the tree\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate (eta)\n",
    "    'subsample': uniform(0.6, 0.4),  # Fraction of samples to use per tree\n",
    "    'colsample_bytree': uniform(0.6, 0.4),  # Fraction of features to use per tree\n",
    "    'min_child_weight': randint(1, 10),  # Minimum sum of instance weight (Hessian)\n",
    "    'gamma': uniform(0, 0.5),  # Minimum loss reduction required to make a further partition\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    xgb_model, param_distributions=param_dist, n_iter=20, cv=3,\n",
    "    verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model with RandomizedSearchCV\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator and print the best parameters\n",
    "best_xgb_model = random_search_xgb.best_estimator_\n",
    "print(\"Best Parameters for XGBoost:\", random_search_xgb.best_params_)\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "109c80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model Accuracy: 0.957625434674615\n",
      "Stacked Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.96      0.96     19442\n",
      "        True       0.96      0.96      0.96     20818\n",
      "\n",
      "    accuracy                           0.96     40260\n",
      "   macro avg       0.96      0.96      0.96     40260\n",
      "weighted avg       0.96      0.96      0.96     40260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the base models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Combine the base models and meta-model in a StackingClassifier\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('rf', rf_model), ('xgb', xgb_model)],\n",
    "    final_estimator=meta_model,  # Meta-model\n",
    "    cv=5,  # 5-fold cross-validation to fit meta-model\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the stacked model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacked_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Stacked Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Stacked Model Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
